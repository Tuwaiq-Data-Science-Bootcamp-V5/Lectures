{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f676d7f",
   "metadata": {},
   "source": [
    "# Text Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70e456",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fce1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'my', 'name', 'is', 'hamad', ',', 'and', 'i', 'am', 'an', 'instructor', 'of', 'ds', 'bootcamp', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Hello, my name is Hamad, and i am an instructor of DS Bootcamp!'\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39503ece",
   "metadata": {},
   "source": [
    "### 1.2 Why Tokenization Instead of Normal Split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdb3464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Python Split Method :\n",
      "['hello,', 'my', 'name', 'is', 'hamad!']\n",
      "\n",
      "Using NLTK word_tokenize  :\n",
      "['hello', ',', 'my', 'name', 'is', 'hamad', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'Hello, my name is Hamad!'\n",
    "\n",
    "split = text.lower().split(' ')\n",
    "\n",
    "print(f'Using Python Split Method :\\n{split}\\n')\n",
    "\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "print(f'Using NLTK word_tokenize  :\\n{tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c5e9ad",
   "metadata": {},
   "source": [
    "### 2 Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4b8c49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentences of the document :\n",
      "['Mr. Hamad is the best.', 'U.S.A is a north American country.']\n",
      "\n",
      "The sentence 1 tokens : ['mr.', 'hamad', 'is', 'the', 'best', '.']\n",
      "The sentence 2 tokens : ['u.s.a', 'is', 'a', 'north', 'american', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "document = \"Mr. Hamad is the best. U.S.A is a north American country.\"\n",
    "sentences = sent_tokenize(document)\n",
    "\n",
    "print(f'The sentences of the document :\\n{sentences}\\n')\n",
    "\n",
    "for i, sent in enumerate(sentences):\n",
    "    tokens = word_tokenize(sent.lower())\n",
    "    print(f'The sentence {i+1} tokens : {tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4774e9",
   "metadata": {},
   "source": [
    "### 3.1 Stemming and Lemmatization : Example One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9432bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr to\n",
      "[nltk_data]     C:\\Users\\Hamad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package udhr is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens before stemming and lemmatization :-\n",
      "['Universal', 'Declaration', 'of', 'Human', 'Rights', 'Preamble', 'Whereas', 'recognition', 'of', 'the']\n",
      "\n",
      "The tokens after stemming :-\n",
      "['univers', 'declar', 'of', 'human', 'right', 'preambl', 'wherea', 'recognit', 'of', 'the']\n",
      "\n",
      "The tokens after lemmatization :-\n",
      "['universal', 'declaration', 'of', 'human', 'right', 'preamble', 'whereas', 'recognition', 'of', 'the']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import udhr\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"udhr\")\n",
    "\n",
    "udhr_data = udhr.sents('English-Latin1')\n",
    "\n",
    "tokens = udhr_data[0][:10]\n",
    "print(f'The tokens before stemming and lemmatization :-\\n{tokens}')\n",
    "print()\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(token.lower()) for token in tokens]\n",
    "print(f'The tokens after stemming :-\\n{stems}')\n",
    "print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmas = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "print(f'The tokens after lemmatization :-\\n{lemmas}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e6ae01",
   "metadata": {},
   "source": [
    "### 4 Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1835783f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text :\n",
      "Dude!! check this video, it is the funniest video ever https://www.youtube.com/watch?v=MN-cmJ7T1Do\n",
      "\n",
      "The text after using RE :\n",
      "Dude!! check this video, it is the funniest video ever \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Dude!! check this video, it is the funniest video ever \" \\\n",
    "        \"https://www.youtube.com/watch?v=MN-cmJ7T1Do\"\n",
    "print(f'The original text :\\n{text}\\n')\n",
    "\n",
    "s = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "\n",
    "print(f'The text after using RE :\\n{s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f17cb31",
   "metadata": {},
   "source": [
    "### 5.1 Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e42b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tokens before removing stop words :\n",
      "['Hamad', 'is', 'an', 'assistant', 'instructor', 'at', 'Tuwaiq', 'Academy', '.']\n",
      "\n",
      "The tokens after removing stop words :\n",
      "['Hamad', 'assistant', 'instructor', 'Tuwaiq', 'Academy', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "text = 'Hamad is an assistant instructor at Tuwaiq Academy.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f'The tokens before removing stop words :\\n{tokens}\\n')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_tokens = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "print(f'The tokens after removing stop words :\\n{filtered_tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb8f7ed",
   "metadata": {},
   "source": [
    "### 5.2 Remove Punctuation Marks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f29ee905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens before removing punctuation marks :\n",
      "['I', 'ca', \"n't\", 'belief', 'that', 'NLP', 'is', 'to', 'easy', '.']\n",
      "\n",
      "Tokens before removing punctuation marks :\n",
      "['I', 'ca', 'belief', 'that', 'NLP', 'is', 'to', 'easy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I can't belief that NLP is to easy.\"\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "print(f'Tokens before removing punctuation marks :\\n{tokens}\\n')\n",
    "\n",
    "tokens = [token for token in tokens if token.isalpha()]\n",
    "\n",
    "print(f'Tokens before removing punctuation marks :\\n{tokens}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699e569",
   "metadata": {},
   "source": [
    "# Generalized Text Preprocessing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed166d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    "import string\n",
    "\n",
    "def process_text(text):\n",
    "    \n",
    "    # Here before tokenization, you can remove starts by text cleaning.\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    final_text = []\n",
    "    for token in tokens:\n",
    "        if (token not in stopwords_english and token not in string.punctuation):\n",
    "            stem_token = ps.stem(token)\n",
    "            final_text.append(stem_token)\n",
    "\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b065d27d",
   "metadata": {},
   "source": [
    "# Numerical Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a53df",
   "metadata": {},
   "source": [
    "### 1 Count Vectorizer (Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a93bcd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>danger</th>\n",
       "      <th>hamad</th>\n",
       "      <th>hello</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hello, World!</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamad is the Best</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am the Danger!</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   best  danger  hamad  hello  world\n",
       "Hello, World!         0       0      0      1      1\n",
       "Hamad is the Best     1       0      1      0      0\n",
       "I am the Danger!      0       1      0      0      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "original_documents = [\n",
    "    'Hello, World!',\n",
    "    'Hamad is the Best',\n",
    "    'I am the Danger!'\n",
    "]\n",
    "\n",
    "preprocessed_documents = []\n",
    "for i, text in enumerate(original_documents):\n",
    "    preprocessed_documents.append(' '.join(process_text(text)))\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "count_vectorized = pd.DataFrame(X.toarray(), index=original_documents, columns=vectorizer.get_feature_names_out())\n",
    "count_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906410b",
   "metadata": {},
   "source": [
    "### 2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2e3f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best</th>\n",
       "      <th>danger</th>\n",
       "      <th>do</th>\n",
       "      <th>hamad</th>\n",
       "      <th>hello</th>\n",
       "      <th>peopl</th>\n",
       "      <th>smoke</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hello, World!</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627914</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.778283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hamad is the Best</th>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I am the Danger!</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Hello people!</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.627914</td>\n",
       "      <td>0.778283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Do not smoke</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       best  danger        do     hamad     hello     peopl  \\\n",
       "Hello, World!      0.000000     0.0  0.000000  0.000000  0.627914  0.000000   \n",
       "Hamad is the Best  0.707107     0.0  0.000000  0.707107  0.000000  0.000000   \n",
       "I am the Danger!   0.000000     1.0  0.000000  0.000000  0.000000  0.000000   \n",
       "Hello people!      0.000000     0.0  0.000000  0.000000  0.627914  0.778283   \n",
       "Do not smoke       0.000000     0.0  0.707107  0.000000  0.000000  0.000000   \n",
       "\n",
       "                      smoke     world  \n",
       "Hello, World!      0.000000  0.778283  \n",
       "Hamad is the Best  0.000000  0.000000  \n",
       "I am the Danger!   0.000000  0.000000  \n",
       "Hello people!      0.000000  0.000000  \n",
       "Do not smoke       0.707107  0.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "original_documents = [\n",
    "    'Hello, World!',\n",
    "    'Hamad is the Best',\n",
    "    'I am the Danger!',\n",
    "    'Hello people!',\n",
    "    'Do not smoke'\n",
    "]\n",
    "\n",
    "preprocessed_documents = []\n",
    "for i, text in enumerate(original_documents):\n",
    "    preprocessed_documents.append(' '.join(process_text(text)))\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_documents)\n",
    "\n",
    "count_vectorized = pd.DataFrame(X.toarray(), index=original_documents, columns=vectorizer.get_feature_names_out())\n",
    "count_vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1fde2",
   "metadata": {},
   "source": [
    "### 3 Positive and Negative Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aa88ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'i am happy because i am learning NLP',\n",
    "    'i love you so much much',\n",
    "    'i am Sad because i am not learning NLP',\n",
    "    'i hate you so much'\n",
    "]\n",
    "\n",
    "labels = [1, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9d8493",
   "metadata": {},
   "source": [
    "#### 3.1 Vocabulary and Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38bebe1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frequencies(corpus, labels):\n",
    "    freqs = {}\n",
    "\n",
    "    for text, y in zip(corpus, labels):\n",
    "\n",
    "        cleaned_text = process_text(text)\n",
    "\n",
    "        for token in cleaned_text:\n",
    "            pair = (token, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    \n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6c0f550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment : Positive - Word : happi - Appeared : 1\n",
      "Sentiment : Positive - Word : learn - Appeared : 1\n",
      "Sentiment : Positive - Word : nlp - Appeared : 1\n",
      "Sentiment : Positive - Word : love - Appeared : 1\n",
      "Sentiment : Positive - Word : much - Appeared : 2\n",
      "Sentiment : Negative - Word : sad - Appeared : 1\n",
      "Sentiment : Negative - Word : learn - Appeared : 1\n",
      "Sentiment : Negative - Word : nlp - Appeared : 1\n",
      "Sentiment : Negative - Word : hate - Appeared : 1\n",
      "Sentiment : Negative - Word : much - Appeared : 1\n"
     ]
    }
   ],
   "source": [
    "freqs = count_frequencies(corpus, labels)\n",
    "\n",
    "for pair in freqs:\n",
    "    sentiment = 'Positive' if pair[1] == 1 else 'Negative'\n",
    "    print(f'Sentiment : {sentiment} - Word : {pair[0]} - Appeared : {freqs[pair]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e812e7",
   "metadata": {},
   "source": [
    "#### 3.2 Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7b16e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, freqs, label):\n",
    "    \n",
    "    # Reprocess the text again\n",
    "    tokens = process_text(text)\n",
    "    \n",
    "    dic = {'Text':text, 'Pos':0, 'Neg':0, 'Label':label}\n",
    "    \n",
    "    for token in tokens:\n",
    "        dic['Pos'] += freqs.get((token, 1.0), 0)\n",
    "        dic['Neg'] += freqs.get((token, 0.0), 0)\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e146457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am happy because i am learning NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love you so much much</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am Sad because i am not learning NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i hate you so much</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Text Pos Neg Label\n",
       "0    i am happy because i am learning NLP   3   2     1\n",
       "1                 i love you so much much   5   2     1\n",
       "2  i am Sad because i am not learning NLP   2   3     0\n",
       "3                      i hate you so much   2   2     0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({}, columns=['Text', 'Pos', 'Neg', 'Label'])\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    df.loc[i, :] = extract_features(corpus[i], freqs, labels[i])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581ec4d0",
   "metadata": {},
   "source": [
    "#### 3.3 FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "431461e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i am happy because i am learning NLP', 'i love you so much much']\n",
      "['i am Sad because i am not learning NLP', 'i hate you so much']\n"
     ]
    }
   ],
   "source": [
    "positive_corpus = corpus[:2]\n",
    "print(positive_corpus)\n",
    "negative_corpus = corpus[2:]\n",
    "print(negative_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fffb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b89cec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_frequencies = []\n",
    "negative_frequencies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7f8a88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happi', 'learn', 'nlp', 'love', 'much', 'much']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "for txt in positive_corpus:\n",
    "    lst = process_text(txt)\n",
    "    for i in lst:\n",
    "        positive_frequencies.append(i)\n",
    "\n",
    "print(positive_frequencies)\n",
    "print(len(positive_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d59fa050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'learn', 'nlp', 'hate', 'much']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "for txt in negative_corpus:\n",
    "    lst = process_text(txt)\n",
    "    for i in lst:\n",
    "        negative_frequencies.append(i)\n",
    "\n",
    "print(negative_frequencies)\n",
    "print(len(negative_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca8c72e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['happi', 'learn', 'nlp', 'love', 'much', 'much']\n",
      "happi 1\n",
      "learn 1\n",
      "nlp 1\n",
      "love 1\n",
      "much 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('happi', 1): 1,\n",
       " ('learn', 1): 1,\n",
       " ('nlp', 1): 1,\n",
       " ('love', 1): 1,\n",
       " ('much', 1): 2}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(positive_frequencies)\n",
    "fdist = dict(FreqDist(positive_frequencies))\n",
    "pos_keys = []\n",
    "for i in fdist:\n",
    "    pos_keys.append((i, 1))\n",
    "    print(i, fdist[i])\n",
    "\n",
    "positive_frequencies = dict(zip(pos_keys, fdist.values()))\n",
    "positive_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "906fa4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sad', 'learn', 'nlp', 'hate', 'much']\n",
      "sad 1\n",
      "learn 1\n",
      "nlp 1\n",
      "hate 1\n",
      "much 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('sad', 0): 1, ('learn', 0): 1, ('nlp', 0): 1, ('hate', 0): 1, ('much', 0): 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(negative_frequencies)\n",
    "fdist = dict(FreqDist(negative_frequencies))\n",
    "neg_keys = []\n",
    "for i in fdist:\n",
    "    neg_keys.append((i, 0))\n",
    "    print(i, fdist[i])\n",
    "\n",
    "negative_frequencies = dict(zip(neg_keys, fdist.values()))\n",
    "negative_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45ccf2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs2 = dict(zip(\n",
    "    list(positive_frequencies.keys()) + list(negative_frequencies.keys()),\n",
    "    list(positive_frequencies.values()) + list(negative_frequencies.values())\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2f4e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({}, columns=['Text', 'Pos', 'Neg', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "89e13e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Pos</th>\n",
       "      <th>Neg</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am happy because i am learning NLP</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love you so much much</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i am Sad because i am not learning NLP</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i hate you so much</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Text Pos Neg Label\n",
       "0    i am happy because i am learning NLP   3   2     1\n",
       "1                 i love you so much much   5   2     1\n",
       "2  i am Sad because i am not learning NLP   2   3     0\n",
       "3                      i hate you so much   2   2     0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({}, columns=['Text', 'Pos', 'Neg', 'Label'])\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    df.loc[i, :] = extract_features(corpus[i], freqs2, labels[i])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05dfc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
